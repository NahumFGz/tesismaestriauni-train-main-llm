{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Annotated, Optional, List, Tuple\n",
    "from uuid import uuid4\n",
    "\n",
    "from langchain.chat_models import init_chat_model\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "from langchain_core.messages import AIMessage, HumanMessage, SystemMessage\n",
    "from langchain_mcp_adapters.client import MultiServerMCPClient\n",
    "from langgraph.graph import END, START, StateGraph\n",
    "from langgraph.graph.message import add_messages\n",
    "from langgraph.prebuilt import ToolNode, tools_condition\n",
    "from typing_extensions import TypedDict\n",
    "from langchain_core.messages import trim_messages\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "\n",
    "memory = MemorySaver()\n",
    "\n",
    "class ChatState(TypedDict):\n",
    "    raw_messages: Annotated[list, add_messages]\n",
    "    messages: Annotated[list, add_messages]\n",
    "    topic_decision: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_rewriter = init_chat_model(\"openai:gpt-4o-mini\", temperature=0.0)\n",
    "llm_classifier = init_chat_model(\"openai:gpt-4o-mini\", temperature=0.0)\n",
    "llm_main = init_chat_model(\"openai:gpt-4o\", temperature=0.5, streaming=True)\n",
    "llm_fallback = init_chat_model(\"openai:gpt-4o-mini\", temperature=0.5, streaming=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_context_for_classification(messages, strategy=\"last\", max_chars=1000):\n",
    "    \"\"\"\n",
    "    Trim rápido usando caracteres como proxy de tokens\n",
    "    Estimación: ~4 caracteres por token en español\n",
    "    \"\"\"\n",
    "    if len(messages) <= 5:\n",
    "        return messages\n",
    "    \n",
    "    # Función inline para contar caracteres\n",
    "    def count_chars(msgs):\n",
    "        return sum(len(msg.content) for msg in msgs)\n",
    "    \n",
    "    trimmer = trim_messages(\n",
    "        max_tokens=max_chars,\n",
    "        strategy=strategy, \n",
    "        token_counter=count_chars,\n",
    "        include_system=False,\n",
    "        allow_partial=False,\n",
    "        start_on=\"human\",\n",
    "    )\n",
    "    \n",
    "    return trimmer.invoke(messages)\n",
    "\n",
    "\n",
    "\n",
    "def format_history_context(messages: List, max_chars: int = 200, exclude_last: bool = True, last_n: int = 5) -> str:\n",
    "    \"\"\"\n",
    "    Formatea el historial en pares Q: A: excluyendo la última pregunta.\n",
    "    - Agrupa en pares (HumanMessage, AIMessage).\n",
    "    - Muestra pregunta completa con prefijo Q:.\n",
    "    - Trunca cada respuesta a max_chars con prefijo A: y añade '…'.\n",
    "    - Cada par en una línea separada.\n",
    "    - Solo incluye los últimos last_n pares.\n",
    "    \"\"\"\n",
    "    pairs: List[Tuple[HumanMessage, Optional[AIMessage]]] = []\n",
    "    pending_human = None\n",
    "    \n",
    "    # Si exclude_last=True, no procesamos el último mensaje\n",
    "    messages_to_process = messages[:-1] if exclude_last and messages else messages\n",
    "\n",
    "    for msg in messages_to_process:\n",
    "        if isinstance(msg, HumanMessage):\n",
    "            # Si ya había un HumanMessage pendiente, lo guardamos sin respuesta\n",
    "            if pending_human is not None:\n",
    "                pairs.append((pending_human, None))\n",
    "            pending_human = msg\n",
    "        elif isinstance(msg, AIMessage) and pending_human is not None:\n",
    "            pairs.append((pending_human, msg))\n",
    "            pending_human = None\n",
    "    \n",
    "    # Guardar el último HumanMessage si quedó pendiente\n",
    "    if pending_human is not None:\n",
    "        pairs.append((pending_human, None))\n",
    "\n",
    "    # Tomar solo los últimos last_n pares\n",
    "    recent_pairs = pairs[-last_n:] if last_n > 0 else pairs\n",
    "\n",
    "    lines = []\n",
    "    for human, ai in recent_pairs:\n",
    "        # Pregunta completa con prefijo Q:\n",
    "        q = human.content.strip()\n",
    "        lines.append(f\"Q: {q}\")\n",
    "\n",
    "        # Respuesta truncada con prefijo A: (si existe)\n",
    "        if ai is not None:\n",
    "            a = ai.content.strip()\n",
    "            if len(a) > max_chars:\n",
    "                # Buscar el último espacio antes del límite para no cortar palabras\n",
    "                cut_point = a.rfind(' ', 0, max_chars)\n",
    "                if cut_point == -1:\n",
    "                    cut_point = max_chars\n",
    "                snippet = a[:cut_point].rstrip() + \"…\"\n",
    "            else:\n",
    "                snippet = a\n",
    "            lines.append(f\"A: {snippet}\")\n",
    "        else:\n",
    "            # Si no hay respuesta, indicamos que no hay respuesta\n",
    "            lines.append(\"A: [Pendiente de responder...]\")\n",
    "        \n",
    "        # Línea vacía entre pares para separar\n",
    "        lines.append(\"\")\n",
    "\n",
    "    return \"\\n\".join(lines).strip()\n",
    "\n",
    "def get_last_question(messages: List) -> str:\n",
    "    \"\"\"\n",
    "    Obtiene la última pregunta del usuario de la lista de mensajes.\n",
    "    \"\"\"\n",
    "    if not messages:\n",
    "        return \"\"\n",
    "    \n",
    "    # Buscar el último HumanMessage\n",
    "    for msg in reversed(messages):\n",
    "        if isinstance(msg, HumanMessage):\n",
    "            return msg.content.strip()\n",
    "    \n",
    "    return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewriter_msg = SystemMessage(\n",
    "    content=(\n",
    "        \"Eres un asistente especializado en reescribir preguntas para alinearlas con el contexto de transparencia gubernamental del Estado peruano. \"\n",
    "        \"Tu objetivo es transformar las preguntas del usuario en consultas más claras, formales y orientadas a la fiscalización y el acceso a información pública.\"\n",
    "\n",
    "        \"Reescribe las preguntas del usuario únicamente si están relacionadas con alguno de los siguientes temas:\"\n",
    "        \"- Contrataciones públicas (montos, órdenes de servicio, contratos, proveedores)\"\n",
    "        \"- Empresas que han contratado con el Estado peruano\"\n",
    "        \"- Asistencia y votaciones de congresistas\"\n",
    "        \"- Información relacionada a congresistas (identidad, región, actividad legislativa)\"\n",
    "\n",
    "        \"Si la pregunta **no** está relacionada con estos temas, **devuélvela sin cambios**.\"\n",
    "        \"Tu respuesta debe ser únicamente la **pregunta reformulada**, sin explicaciones ni comentarios adicionales.\"\n",
    "        \n",
    "        \"Ejemplos:\"\n",
    "        \"Entrada: quien es alejando muñante\"\n",
    "        \"Salida: Busca en la web información sobre el congresista ALEJANDRO MUÑANTE.\"\n",
    "\n",
    "        \"Entrada: quien es Sucel Paredes\"\n",
    "        \"Salida: Busca en la web información sobre la congresista SUCEL PAREDES.\"\n",
    "\n",
    "        \"Entrada: quienes son los congresistas de la region de huancayo\"\n",
    "        \"Salida: Busca en la web información sobre los congresistas de la región de HUANCAYO.\"\n",
    "        \n",
    "        \"Entrada: dame las asistencias del 2022 octubre\"\n",
    "        \"Salida: ¿Cuáles fueron las asistencias de los congresistas en octubre de 2022?\"\n",
    "\n",
    "        \"Entrada: puedes darme las asistencias del 10 de diciembre del 2022\"\n",
    "        \"Salida: ¿Cuáles fueron las asistencias de los congresistas el 2022-12-10?\"\n",
    "\n",
    "        \"Entrada: puedes decirme las votaciones del congreso del 10 de diciembre del 2022\"\n",
    "        \"Salida: ¿Cuáles fueron las votaciones de los congresistas el 2022-12-10?\"\n",
    "\n",
    "        \"Entrada: que asuntos se trataron en el congreso del 10 de diciembre del 2022\"\n",
    "        \"Salida: ¿Cuáles fueron los asuntos tratados en las votaciones del 2022-12-10?\"\n",
    "\n",
    "        \"Entrada: cuánto ha contratado constructora alfa\"\n",
    "        \"Salida: ¿Cuánto ha contratado la empresa 'CONSTRUCTORA ALFA' con el Estado peruano según transparencia pública?\"\n",
    "\n",
    "        \"Entrada: detalles de los contratos de constructora alfa\"\n",
    "        \"Salida: ¿Cuáles son los detalles de los contratos de la empresa 'CONSTRUCTORA ALFA'?\"\n",
    "\n",
    "        \"Entrada: detalles de las ordenes de servicio de constructora alfa\"\n",
    "        \"Salida: ¿Cuáles son los detalles de las órdenes de servicio de la empresa 'CONSTRUCTORA ALFA'?\"\n",
    "\n",
    "        \"Entrada: Que más me puedes decir?\"\n",
    "        \"Salida: Que más me puedes decir?\"\n",
    "\n",
    "        \"Entrada: Me gustan los duraznos\"\n",
    "        \"Salida: Me gustan los duraznos\"\n",
    "\n",
    "        \"Entrada: Quien ganó la champions league\"\n",
    "        \"Salida: Quien ganó la champions league\"\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rewrite_node(state: ChatState) -> ChatState:\n",
    "    \"\"\"Reescribe la última pregunta del usuario con llm_rewriter.\"\"\"\n",
    "    \n",
    "    last_user_msg: HumanMessage = state[\"raw_messages\"][-1]   # asumimos último mensaje = usuario\n",
    "    rewritten = llm_rewriter.invoke([rewriter_msg, last_user_msg])\n",
    "\n",
    "    # Añadimos la versión reescrita como HumanMessage (para mantener formato)\n",
    "    return {\n",
    "        \"raw_messages\": state[\"raw_messages\"],\n",
    "        \"messages\": state[\"messages\"] + [HumanMessage(content=rewritten.content)],\n",
    "        \"topic_decision\": state.get(\"topic_decision\", \"\")\n",
    "    }\n",
    "\n",
    "def classifier_node(state: ChatState) -> ChatState:\n",
    "    \"\"\"Clasifica si la conversación está relacionada con transparencia gubernamental.\"\"\"\n",
    "    \n",
    "    # Usamos `messages` (el output del rewriter) para el contexto\n",
    "    msgs_for_context = state[\"messages\"]\n",
    "    \n",
    "    # Obtener contexto histórico (excluyendo la última pregunta)\n",
    "    history_context = format_history_context(msgs_for_context, max_chars=200, exclude_last=True, last_n=4)\n",
    "    \n",
    "    # Obtener la última pregunta\n",
    "    last_question = get_last_question(msgs_for_context)\n",
    "    \n",
    "    print(\"--------------------------------\")\n",
    "    print(\"CONTEXTO HISTÓRICO:\")\n",
    "    print(history_context)\n",
    "    print(\"ÚLTIMA PREGUNTA:\")\n",
    "    print(last_question)\n",
    "    print(\"--------------------------------\")\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "    Eres un verificador que decide si la última pregunta del usuario puede ser respondida en el contexto de transparencia gubernamental del Estado peruano.\n",
    "\n",
    "    TEMAS RELEVANTES:\n",
    "    - Contrataciones públicas (montos, órdenes de servicio, contratos, proveedores)\n",
    "    - Empresas que han contratado con el Estado peruano\n",
    "    - Asistencia y votaciones de congresistas\n",
    "    - Información relacionada a congresistas (identidad, región, actividad legislativa)\n",
    "    - Transparencia y fiscalización gubernamental en general\n",
    "\n",
    "    INSTRUCCIONES:\n",
    "    - Si el contexto histórico muestra que el usuario estuvo hablando de los TEMAS RELEVANTES que se muestran arriba, responde 'YES'.\n",
    "    - Si la última pregunta es sobre un tema totalmente distinto al contexto histórico, responde 'NO'.\n",
    "    - Si no hay contexto histórico, evalúa solo si la última pregunta es sobre los TEMAS RELEVANTES.\n",
    "\n",
    "    Solo responde con 'YES' o 'NO' (sin explicaciones ni comentarios adicionales).\n",
    "\n",
    "    CONTEXTO HISTÓRICO:\n",
    "    {history_context}\n",
    "\n",
    "    ÚLTIMA PREGUNTA:\n",
    "    {last_question}\n",
    "\n",
    "    ¿Se puede responder la última pregunta en el contexto de los TEMAS RELEVANTES?\n",
    "    \"\"\"\n",
    "\n",
    "    classification = llm_classifier.invoke([HumanMessage(content=prompt)])\n",
    "    decision = classification.content.strip().upper()\n",
    "    if decision not in (\"YES\", \"NO\"):\n",
    "        decision = \"NO\"\n",
    "\n",
    "    print(f\"DECISIÓN DEL CLASIFICADOR: {decision}\")\n",
    "    print(\"================================\")\n",
    "\n",
    "    return {\n",
    "        \"raw_messages\": state[\"raw_messages\"],\n",
    "        \"messages\": state[\"messages\"],\n",
    "        \"topic_decision\": decision\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = StateGraph(ChatState)\n",
    "\n",
    "graph.add_node(\"rewriter\", rewrite_node)\n",
    "graph.add_node(\"classifier\", classifier_node)\n",
    "\n",
    "# Definir el flujo: input -> rewriter -> classifier -> end\n",
    "graph.add_edge(START, \"rewriter\")\n",
    "graph.add_edge(\"rewriter\", \"classifier\")\n",
    "graph.add_edge(\"classifier\", END)\n",
    "\n",
    "enhanced_processor = graph.compile(checkpointer=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejemplo de uso\n",
    "session_id = \"abc123\"\n",
    "\n",
    "# Test 1: Pregunta sobre transparencia gubernamental (debe ir a main)\n",
    "print(\"=== Test 1: Pregunta sobre contrataciones ===\")\n",
    "state = enhanced_processor.invoke(\n",
    "    {\n",
    "        \"raw_messages\": [HumanMessage(content=\"Que empresas tienen contratos por más de 1000000 de soles\")],\n",
    "        \"topic_decision\": \"\"\n",
    "    },\n",
    "    config={\"thread_id\": session_id}\n",
    ")\n",
    "print(f\"Pregunta original: {state['raw_messages'][-1].content}\")\n",
    "print(f\"Pregunta reescrita: {state['messages'][-1].content}\")\n",
    "print(f\"Clasificación: {state['topic_decision']}\")\n",
    "print()\n",
    "\n",
    "# Test 2: Pregunta general (debe ir a fallback)\n",
    "print(\"=== Test 2: Pregunta general ===\")\n",
    "state = enhanced_processor.invoke(\n",
    "    {\n",
    "        \"raw_messages\": [HumanMessage(content=\"Quien ganó la Champions League\")],\n",
    "        \"topic_decision\": \"\"\n",
    "    },\n",
    "    config={\"thread_id\": session_id}  \n",
    ")\n",
    "print(f\"Pregunta original: {state['raw_messages'][-1].content}\")\n",
    "print(f\"Pregunta reescrita: {state['messages'][-1].content}\")\n",
    "print(f\"Clasificación: {state['topic_decision']}\")\n",
    "print()\n",
    "\n",
    "# Test 3: Pregunta sobre asistencias (debe ir a main)\n",
    "print(\"=== Test 3: Pregunta sobre asistencias ===\")\n",
    "state = enhanced_processor.invoke(\n",
    "    {\n",
    "        \"raw_messages\": [HumanMessage(content=\"dame las asistencias del 2022 octubre\")],\n",
    "        \"topic_decision\": \"\"\n",
    "    },\n",
    "    config={\"thread_id\": session_id}  \n",
    ")\n",
    "print(f\"Pregunta original: {state['raw_messages'][-1].content}\")\n",
    "print(f\"Pregunta reescrita: {state['messages'][-1].content}\")\n",
    "print(f\"Clasificación: {state['topic_decision']}\")\n",
    "print()\n",
    "\n",
    "# Test 4: Pregunta sobre que mas puedes hacer\n",
    "print(\"=== Test 4:  sobre que mas puedes hacer ===\")\n",
    "state = enhanced_processor.invoke(\n",
    "    {\n",
    "        \"raw_messages\": [HumanMessage(content=\"que mas puedes hacer\")],\n",
    "        \"topic_decision\": \"\"\n",
    "    },\n",
    "    config={\"thread_id\": session_id}  \n",
    ")\n",
    "print(f\"Pregunta original: {state['raw_messages'][-1].content}\")\n",
    "print(f\"Pregunta reescrita: {state['messages'][-1].content}\")\n",
    "print(f\"Clasificación: {state['topic_decision']}\")\n",
    "print()\n",
    "\n",
    "# Test 5: Pregunta sobre los contratos de la empresa constructora alfa\n",
    "print(\"=== Test 5: Pregunta sobre los contratos de la empresa constructora alfa ===\")\n",
    "state = enhanced_processor.invoke(\n",
    "    {\n",
    "        \"raw_messages\": [HumanMessage(content=\"detalles de los contratos de constructora alfa\")],\n",
    "        \"topic_decision\": \"\"\n",
    "    },\n",
    "    config={\"thread_id\": session_id}  \n",
    ")\n",
    "print(f\"Pregunta original: {state['raw_messages'][-1].content}\")\n",
    "print(f\"Pregunta reescrita: {state['messages'][-1].content}\")\n",
    "print(f\"Clasificación: {state['topic_decision']}\")\n",
    "print()\n",
    "\n",
    "# Test 5: Pregunta sobre que mas puedes hacer\n",
    "print(\"=== Test 5:  sobre que mas sabes de la empresa ===\")\n",
    "state = enhanced_processor.invoke(\n",
    "    {\n",
    "        \"raw_messages\": [HumanMessage(content=\"que mas sabes de la empresa\")],\n",
    "        \"topic_decision\": \"\"\n",
    "    },\n",
    "    config={\"thread_id\": session_id}  \n",
    ")\n",
    "print(f\"Pregunta original: {state['raw_messages'][-1].content}\")\n",
    "print(f\"Pregunta reescrita: {state['messages'][-1].content}\")\n",
    "print(f\"Clasificación: {state['topic_decision']}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "display(Image(enhanced_processor.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_budget",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
